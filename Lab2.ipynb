{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDjgcs/FjawNu4jEiRsc3l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tripti-bhardwaj/Deep_Learning_Lab/blob/main/Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbWfiJ_F4IYU"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for the neural network\n",
        "input_size = 784 # 28x28 pixel images\n",
        "hidden_size = 64 # Number of neurons in the hidden layer\n",
        "output_size = 10 # 10 classes for digits 0-9\n",
        "learning_rate = 0.01 # Learning rate for gradient descent\n",
        "epochs = 5000 # Number of training epochs"
      ],
      "metadata": {
        "id": "hd-9UEhg8VrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST training and testing datasets from CSV files\n",
        "train_data = pd.read_csv(\"/content/mnist_train.csv\")\n",
        "test_data = pd.read_csv(\"/content/mnist_test.csv\")"
      ],
      "metadata": {
        "id": "UBCFY4IR4eON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the training data:\n",
        "# - Separate features (X_train) from labels (y_train)\n",
        "# - Fill any NaN values with 0 (representing black pixels)\n",
        "# - Normalize pixel values by dividing by 255.0 (max pixel value)\n",
        "X_train = train_data.drop(columns=['label']).fillna(0).values / 255.0\n",
        "y_train = train_data['label'].values\n",
        "\n",
        "# Prepare the testing data similarly\n",
        "X_test = test_data.drop(columns=['label']).fillna(0).values / 255.0\n",
        "y_test = test_data['label'].values"
      ],
      "metadata": {
        "id": "vRt0Wfvx8Di9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert integer labels to one-hot encoded vectors\n",
        "# For example, label 3 becomes [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "y_train = np.eye(10)[y_train]\n",
        "y_test = np.eye(10)[y_test]"
      ],
      "metadata": {
        "id": "kPQw4dBn8K8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sigmoid activation function\n",
        "# np.clip is used to prevent overflow issues with large 'z' values\n",
        "def sigmoid(z):\n",
        "    z = np.clip(z, -500, 500)\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Define the softmax activation function\n",
        "# Used for output layer in multi-class classification\n",
        "def softmax(z):\n",
        "    # Subtract max for numerical stability to prevent overflow\n",
        "    e = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return e / np.sum(e, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "LJE-OpTb1BZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork():\n",
        "    # Constructor to initialize weights and biases\n",
        "    def __init__(self, input_size, hidden_size, output_size, lr):\n",
        "        # Initialize weights with Xavier initialization (scaled random normal)\n",
        "        self.weight1 = np.random.randn(input_size, hidden_size) * np.sqrt(1 / input_size)\n",
        "        self.bias1 = np.zeros((1, hidden_size))\n",
        "        self.weight2 = np.random.randn(hidden_size, output_size) * np.sqrt(1 / hidden_size)\n",
        "        self.bias2 = np.zeros((1, output_size))\n",
        "        self.lr = lr # Store the learning rate\n",
        "\n",
        "    # Forward propagation calculates the output of the network\n",
        "    def forward_propagation(self, X):\n",
        "        # Layer 1: Linear transformation + sigmoid activation\n",
        "        z1 = np.dot(X, self.weight1) + self.bias1\n",
        "        self.a1 = sigmoid(z1)\n",
        "        # Layer 2: Linear transformation + softmax activation\n",
        "        z2 = np.dot(self.a1, self.weight2) + self.bias2\n",
        "        z2 = np.clip(z2, -50, 50)\n",
        "        a2 = softmax(z2) # Output probabilities\n",
        "        return a2\n",
        "\n",
        "    # Backward propagation calculates gradients and updates weights\n",
        "    def backward_propagation(self, X, y_actual, y_predicted):\n",
        "        m = X.shape[0] # Number of samples\n",
        "\n",
        "        # Output layer gradients\n",
        "        dz2 = y_predicted - y_actual # Error at the output\n",
        "        dweight2 = np.dot(self.a1.T, dz2) / m # Gradient for weight2\n",
        "        dbias2 = np.sum(dz2, axis=0, keepdims=True) / m # Gradient for bias2\n",
        "\n",
        "        # Hidden layer gradients\n",
        "        # dz1 incorporates the derivative of sigmoid activation\n",
        "        dz1 = np.dot(dz2, self.weight2.T) * self.a1 * (1 - self.a1)\n",
        "        dweight1 = np.dot(X.T, dz1) / m # Gradient for weight1\n",
        "        dbias1 = np.sum(dz1, axis=0, keepdims=True) / m # Gradient for bias1\n",
        "\n",
        "        # Clip gradients to prevent exploding gradients\n",
        "        dweight1 = np.clip(dweight1, -5, 5)\n",
        "        dweight2 = np.clip(dweight2, -5, 5)\n",
        "\n",
        "        # Update weights and biases using gradient descent\n",
        "        self.weight1 -= self.lr * dweight1\n",
        "        self.bias1 -= self.lr * dbias1\n",
        "        self.weight2 -= self.lr * dweight2\n",
        "        self.bias2 -= self.lr * dbias2\n",
        "\n",
        "    # Training loop for the neural network\n",
        "    def train(self, X, y, epochs):\n",
        "      for epoch in range(epochs):\n",
        "          y_predicted = self.forward_propagation(X)\n",
        "          # Clip predicted probabilities to avoid log(0) for loss calculation\n",
        "          y_predicted = np.clip(y_predicted, 1e-8, 1 - 1e-8)\n",
        "          self.backward_propagation(X, y, y_predicted)\n",
        "          # Calculate cross-entropy loss\n",
        "          loss = -np.sum(y * np.log(y_predicted)) / y.shape[0]\n",
        "          # Print loss periodically\n",
        "          if (epoch + 1) % 100 == 0:\n",
        "              print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    # Predict class labels for new input data\n",
        "    def predict(self, X):\n",
        "        y_predicted = self.forward_propagation(X)\n",
        "        # Return the index of the highest probability as the predicted class\n",
        "        return np.argmax(y_predicted, axis=1)"
      ],
      "metadata": {
        "id": "RQvOc02G5Vlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the NeuralNetwork class\n",
        "nn = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
        "\n",
        "# Train the neural network using the training data\n",
        "nn.train(X_train, y_train, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUnP5jQ743Pk",
        "outputId": "faad443f-a065-46ed-d4a5-ba27f17a66dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/5000, Loss: 2.2534\n",
            "Epoch 200/5000, Loss: 2.1796\n",
            "Epoch 300/5000, Loss: 2.1100\n",
            "Epoch 400/5000, Loss: 2.0395\n",
            "Epoch 500/5000, Loss: 1.9674\n",
            "Epoch 600/5000, Loss: 1.8935\n",
            "Epoch 700/5000, Loss: 1.8182\n",
            "Epoch 800/5000, Loss: 1.7423\n",
            "Epoch 900/5000, Loss: 1.6669\n",
            "Epoch 1000/5000, Loss: 1.5931\n",
            "Epoch 1100/5000, Loss: 1.5218\n",
            "Epoch 1200/5000, Loss: 1.4537\n",
            "Epoch 1300/5000, Loss: 1.3894\n",
            "Epoch 1400/5000, Loss: 1.3290\n",
            "Epoch 1500/5000, Loss: 1.2727\n",
            "Epoch 1600/5000, Loss: 1.2204\n",
            "Epoch 1700/5000, Loss: 1.1718\n",
            "Epoch 1800/5000, Loss: 1.1268\n",
            "Epoch 1900/5000, Loss: 1.0851\n",
            "Epoch 2000/5000, Loss: 1.0465\n",
            "Epoch 2100/5000, Loss: 1.0107\n",
            "Epoch 2200/5000, Loss: 0.9775\n",
            "Epoch 2300/5000, Loss: 0.9466\n",
            "Epoch 2400/5000, Loss: 0.9178\n",
            "Epoch 2500/5000, Loss: 0.8910\n",
            "Epoch 2600/5000, Loss: 0.8660\n",
            "Epoch 2700/5000, Loss: 0.8426\n",
            "Epoch 2800/5000, Loss: 0.8206\n",
            "Epoch 2900/5000, Loss: 0.8000\n",
            "Epoch 3000/5000, Loss: 0.7806\n",
            "Epoch 3100/5000, Loss: 0.7624\n",
            "Epoch 3200/5000, Loss: 0.7451\n",
            "Epoch 3300/5000, Loss: 0.7289\n",
            "Epoch 3400/5000, Loss: 0.7135\n",
            "Epoch 3500/5000, Loss: 0.6989\n",
            "Epoch 3600/5000, Loss: 0.6851\n",
            "Epoch 3700/5000, Loss: 0.6720\n",
            "Epoch 3800/5000, Loss: 0.6595\n",
            "Epoch 3900/5000, Loss: 0.6476\n",
            "Epoch 4000/5000, Loss: 0.6363\n",
            "Epoch 4100/5000, Loss: 0.6255\n",
            "Epoch 4200/5000, Loss: 0.6152\n",
            "Epoch 4300/5000, Loss: 0.6053\n",
            "Epoch 4400/5000, Loss: 0.5959\n",
            "Epoch 4500/5000, Loss: 0.5868\n",
            "Epoch 4600/5000, Loss: 0.5782\n",
            "Epoch 4700/5000, Loss: 0.5698\n",
            "Epoch 4800/5000, Loss: 0.5618\n",
            "Epoch 4900/5000, Loss: 0.5542\n",
            "Epoch 5000/5000, Loss: 0.5468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_test_predictions = nn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy by comparing predictions with actual labels\n",
        "# np.argmax(y_test, axis=1) converts one-hot encoded y_test back to integer labels\n",
        "accuracy = np.mean(y_test_predictions == np.argmax(y_test, axis=1))\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SwXbD-w5LRL",
        "outputId": "c64302de-be12-4221-833b-1522005df8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 83.13%\n"
          ]
        }
      ]
    }
  ]
}