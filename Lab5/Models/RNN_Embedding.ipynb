{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QV6ts4fTZT6y"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries for data manipulation, neural networks, and visualization.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "JNOL9RUFak-y"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset from a CSV file into a pandas DataFrame.\n",
        "df = pd.read_csv(\"poems-100.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iSpLUoEKao5L"
      },
      "outputs": [],
      "source": [
        "# Concatenating all text from the first column, converting to lowercase, and then splitting into individual tokens (words).\n",
        "text = \" \".join(df.iloc[:, 0].astype(str).tolist()).lower()\n",
        "tokens = text.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb1F7C95asLH",
        "outputId": "bb8fee7a-e6aa-480f-fec4-c871609bd4d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 6989\n"
          ]
        }
      ],
      "source": [
        "# Creating a vocabulary of unique words from the tokens and determining its size.\n",
        "vocab = sorted(set(tokens))\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocabulary Size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "XI_QHI2nauEW"
      },
      "outputs": [],
      "source": [
        "# Creating mappings from words to numerical indices and vice-versa for efficient processing.\n",
        "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx_to_word = {i: w for w, i in word_to_idx.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ZzDbEBfDavy2"
      },
      "outputs": [],
      "source": [
        "# Converting the list of words (tokens) into a list of their corresponding numerical indices.\n",
        "indexed_tokens = [word_to_idx[w] for w in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "DeZI8TfEayIC"
      },
      "outputs": [],
      "source": [
        "# Preparing sequences for training: creating input sequences of a fixed length and their corresponding target words.\n",
        "sequence_length = 5\n",
        "\n",
        "inputs = []\n",
        "targets = []\n",
        "\n",
        "for i in range(len(indexed_tokens) - sequence_length):\n",
        "    inputs.append(indexed_tokens[i:i+sequence_length])\n",
        "    targets.append(indexed_tokens[i+sequence_length])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "CmiIntWMa0S1"
      },
      "outputs": [],
      "source": [
        "# Converting the input sequences and target words into PyTorch tensors.\n",
        "X = torch.tensor(inputs)\n",
        "y = torch.tensor(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "o8gBlXJ_a1ub"
      },
      "outputs": [],
      "source": [
        "# Defining a Recurrent Neural Network (RNN) model with an embedding layer for text generation.\n",
        "class RNN_Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)          # (batch, seq, embed_dim) - Embeds the input indices into dense vectors\n",
        "        out, _ = self.rnn(x)           # (batch, seq, hidden) - Processes the embedded sequences with an RNN layer\n",
        "        out = self.fc(out[:, -1, :])   # last time step - Applies a linear layer to the last output of the RNN to predict the next word\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "U0RXfv4Va3wb"
      },
      "outputs": [],
      "source": [
        "# Initializing model parameters, defining the model architecture, loss function, and optimizer.\n",
        "embed_size = 100\n",
        "hidden_size = 128\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "\n",
        "model = RNN_Embedding(vocab_size, embed_size, hidden_size)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrloZ9Y2a9vu",
        "outputId": "5daa362f-76ce-40ef-8af5-3b82bad9d208"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Loss: 7.3169\n",
            "Epoch 2/50, Loss: 6.2197\n",
            "Epoch 3/50, Loss: 5.6579\n",
            "Epoch 4/50, Loss: 5.1357\n",
            "Epoch 5/50, Loss: 4.6315\n",
            "Epoch 6/50, Loss: 4.1092\n",
            "Epoch 7/50, Loss: 3.5915\n",
            "Epoch 8/50, Loss: 3.1180\n",
            "Epoch 9/50, Loss: 2.6941\n",
            "Epoch 10/50, Loss: 2.3113\n",
            "Epoch 11/50, Loss: 1.9706\n",
            "Epoch 12/50, Loss: 1.6726\n",
            "Epoch 13/50, Loss: 1.4135\n",
            "Epoch 14/50, Loss: 1.1882\n",
            "Epoch 15/50, Loss: 0.9937\n",
            "Epoch 16/50, Loss: 0.8257\n",
            "Epoch 17/50, Loss: 0.6833\n",
            "Epoch 18/50, Loss: 0.5636\n",
            "Epoch 19/50, Loss: 0.4659\n",
            "Epoch 20/50, Loss: 0.3852\n",
            "Epoch 21/50, Loss: 0.3209\n",
            "Epoch 22/50, Loss: 0.2683\n",
            "Epoch 23/50, Loss: 0.2287\n",
            "Epoch 24/50, Loss: 0.1999\n",
            "Epoch 25/50, Loss: 0.1739\n",
            "Epoch 26/50, Loss: 0.1523\n",
            "Epoch 27/50, Loss: 0.1393\n",
            "Epoch 28/50, Loss: 0.1282\n",
            "Epoch 29/50, Loss: 0.1172\n",
            "Epoch 30/50, Loss: 0.1089\n",
            "Epoch 31/50, Loss: 0.1063\n",
            "Epoch 32/50, Loss: 0.0977\n",
            "Epoch 33/50, Loss: 0.0893\n",
            "Epoch 34/50, Loss: 0.0871\n",
            "Epoch 35/50, Loss: 0.0839\n",
            "Epoch 36/50, Loss: 0.0793\n",
            "Epoch 37/50, Loss: 0.0736\n",
            "Epoch 38/50, Loss: 0.0673\n",
            "Epoch 39/50, Loss: 0.0599\n",
            "Epoch 40/50, Loss: 0.0618\n",
            "Epoch 41/50, Loss: 0.0595\n",
            "Epoch 42/50, Loss: 0.0566\n",
            "Epoch 43/50, Loss: 0.0579\n",
            "Epoch 44/50, Loss: 0.0539\n",
            "Epoch 45/50, Loss: 0.0452\n",
            "Epoch 46/50, Loss: 0.0411\n",
            "Epoch 47/50, Loss: 0.0324\n",
            "Epoch 48/50, Loss: 0.0347\n",
            "Epoch 49/50, Loss: 0.0414\n",
            "Epoch 50/50, Loss: 0.0388\n",
            "\n",
            "Training Time: 130.76644897460938 seconds\n"
          ]
        }
      ],
      "source": [
        "# Training loop for the RNN model, iterating through epochs and batches, performing forward and backward passes, and updating model weights. Records loss and training time.\n",
        "losses = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i in range(0, len(X), batch_size):\n",
        "\n",
        "        batch_inputs = X[i:i+batch_size]\n",
        "        batch_targets = y[i:i+batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_inputs)\n",
        "        loss = criterion(outputs, batch_targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    num_batches = (len(X) + batch_size - 1) // batch_size\n",
        "    avg_loss = epoch_loss / num_batches\n",
        "\n",
        "    losses.append(avg_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "embedding_training_time = end_time - start_time\n",
        "embedding_final_loss = losses[-1]\n",
        "\n",
        "print(\"\\nTraining Time:\", embedding_training_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "0_5B_muSbNwa"
      },
      "outputs": [],
      "source": [
        "# Creating a pandas DataFrame to store and save the training results (model, time, loss, and observation) to a CSV file.\n",
        "df = pd.DataFrame([{\n",
        "    \"Model\": \"Trainable Embedding (PyTorch)\",\n",
        "    \"Training_Time_Seconds\": embedding_training_time,\n",
        "    \"Final_Loss\": embedding_final_loss\n",
        "}])\n",
        "\n",
        "df.to_csv(\"embedding_results.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "gckGrP8MbOZN"
      },
      "outputs": [],
      "source": [
        "# Defining a function to generate text using the trained RNN model, starting from a given sequence of words.\n",
        "def generate_text(start_words, length=20):\n",
        "\n",
        "    model.eval()\n",
        "    words = start_words.copy()\n",
        "\n",
        "    for _ in range(length):\n",
        "\n",
        "        seq = [word_to_idx[w] for w in words[-sequence_length:]]\n",
        "        seq_tensor = torch.tensor([seq])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(seq_tensor)\n",
        "\n",
        "        probs = torch.softmax(output, dim=1)\n",
        "        next_word_idx = torch.argmax(probs).item()\n",
        "\n",
        "        words.append(idx_to_word[next_word_idx])\n",
        "\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdLWzjZ9bSWZ",
        "outputId": "3ac53660-76a3-4682-8bfc-6e8949157acb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "o my luve's like a red, red rose that’s newly sprung in june; o my luve's like the melodie that’s sweetly play'd in tune. as fair art thou, my bonnie lass, so deep in luve\n"
          ]
        }
      ],
      "source": [
        "# Generating text using the `generate_text` function, starting with the first `sequence_length` tokens from the dataset.\n",
        "start = tokens[:sequence_length]\n",
        "print(generate_text(start, 30))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
