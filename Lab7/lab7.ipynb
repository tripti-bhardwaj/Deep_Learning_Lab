{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "781e4e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: mps\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "import sacrebleu\n",
    "from typing import Optional\n",
    "\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DATA_PATH = Path(\"spa.txt\")\n",
    "SAMPLE_N = 10_000\n",
    "\n",
    "TRAIN_RATIO = 0.80\n",
    "VAL_RATIO   = 0.10\n",
    "TEST_RATIO  = 0.10\n",
    "\n",
    "LOWERCASE = True\n",
    "MIN_FREQ = 2\n",
    "MAX_VOCAB_SRC = 20_000\n",
    "MAX_VOCAB_TGT = 20_000\n",
    "\n",
    "MAX_LEN_SRC = 60\n",
    "MAX_LEN_TGT = 60\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "D_MODEL = 256\n",
    "DROPOUT = 0.1\n",
    "MAX_LEN = 256\n",
    "\n",
    "BASE_DIR = Path(\"outputs\")\n",
    "TRANS_DIR = Path(\"outputs_transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "785ad52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs in file: 118964\n",
      "Using: 10000\n",
      "Train: 8000 Val: 1000 Test: 1000\n"
     ]
    }
   ],
   "source": [
    "def load_pairs(path: Path) -> List[Tuple[str, str]]:\n",
    "    pairs: List[Tuple[str, str]] = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            en, es = parts[0], parts[1]\n",
    "            pairs.append((en, es))\n",
    "    return pairs\n",
    "\n",
    "def split_pairs(\n",
    "    pairs: List[Tuple[str, str]],\n",
    "    sample_n: Optional[int] = None,\n",
    "    train_ratio: float = 0.8,\n",
    "    val_ratio: float = 0.1,\n",
    "    test_ratio: float = 0.1,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6\n",
    "\n",
    "    rng = random.Random(seed)\n",
    "    pairs = pairs[:]\n",
    "    rng.shuffle(pairs)\n",
    "\n",
    "    if sample_n is not None:\n",
    "        pairs = pairs[:sample_n]\n",
    "\n",
    "    n = len(pairs)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val   = int(n * val_ratio)\n",
    "\n",
    "    train = pairs[:n_train]\n",
    "    val   = pairs[n_train:n_train + n_val]\n",
    "    test  = pairs[n_train + n_val:]\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "pairs = load_pairs(DATA_PATH)\n",
    "train_pairs, val_pairs, test_pairs = split_pairs(\n",
    "    pairs,\n",
    "    sample_n=SAMPLE_N,\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    test_ratio=TEST_RATIO,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(\"Total pairs in file:\", len(pairs))\n",
    "print(\"Using:\", len(train_pairs) + len(val_pairs) + len(test_pairs))\n",
    "print(\"Train:\", len(train_pairs), \"Val:\", len(val_pairs), \"Test:\", len(test_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcc76f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world', '!']\n"
     ]
    }
   ],
   "source": [
    "PAD = \"<pad>\"\n",
    "UNK = \"<unk>\"\n",
    "BOS = \"<bos>\"\n",
    "EOS = \"<eos>\"\n",
    "SPECIAL_TOKENS = [PAD, UNK, BOS, EOS]\n",
    "\n",
    "def normalize(text: str, lowercase: bool = True) -> str:\n",
    "    text = text.strip()\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    text = re.sub(r\"([.,!?;:()\\\"'])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.split() if text else []\n",
    "\n",
    "print(tokenize(normalize(\"Hello, World!   \", True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b8248e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC vocab size: 2279\n",
      "TGT vocab size: 3052\n",
      "pad_id_src: 0 pad_id_tgt: 0 bos_id_tgt: 2 eos_id_tgt: 3\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(\n",
    "    pairs: List[Tuple[str, str]],\n",
    "    side: str,\n",
    "    min_freq: int = 2,\n",
    "    max_vocab: Optional[int] = None,\n",
    "    lowercase: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    assert side in {\"src\", \"tgt\"}\n",
    "    idx = 0 if side == \"src\" else 1\n",
    "\n",
    "    counter = Counter()\n",
    "    for en, es in pairs:\n",
    "        txt = en if idx == 0 else es\n",
    "        toks = tokenize(normalize(txt, lowercase))\n",
    "        counter.update(toks)\n",
    "\n",
    "    itos = list(SPECIAL_TOKENS)\n",
    "    for tok, c in counter.most_common():\n",
    "        if c < min_freq:\n",
    "            break\n",
    "        if tok in SPECIAL_TOKENS:\n",
    "            continue\n",
    "        if max_vocab is not None and len(itos) >= max_vocab:\n",
    "            break\n",
    "        itos.append(tok)\n",
    "\n",
    "    stoi = {t: i for i, t in enumerate(itos)}\n",
    "    return {\"itos\": itos, \"stoi\": stoi}\n",
    "\n",
    "src_vocab = build_vocab(train_pairs, \"src\", min_freq=MIN_FREQ, max_vocab=MAX_VOCAB_SRC, lowercase=LOWERCASE)\n",
    "tgt_vocab = build_vocab(train_pairs, \"tgt\", min_freq=MIN_FREQ, max_vocab=MAX_VOCAB_TGT, lowercase=LOWERCASE)\n",
    "\n",
    "pad_id_src = src_vocab[\"stoi\"][PAD]\n",
    "pad_id_tgt = tgt_vocab[\"stoi\"][PAD]\n",
    "bos_id_tgt = tgt_vocab[\"stoi\"][BOS]\n",
    "eos_id_tgt = tgt_vocab[\"stoi\"][EOS]\n",
    "\n",
    "print(\"SRC vocab size:\", len(src_vocab[\"itos\"]))\n",
    "print(\"TGT vocab size:\", len(tgt_vocab[\"itos\"]))\n",
    "print(\"pad_id_src:\", pad_id_src, \"pad_id_tgt:\", pad_id_tgt, \"bos_id_tgt:\", bos_id_tgt, \"eos_id_tgt:\", eos_id_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57a3429e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: torch.Size([64, 18])\n",
      "tgt_in: torch.Size([64, 16])\n",
      "tgt_out: torch.Size([64, 16])\n",
      "src_key_padding_mask: torch.Size([64, 18]) torch.bool\n",
      "tgt_key_padding_mask: torch.Size([64, 16]) torch.bool\n",
      "tgt_subsequent_mask: torch.Size([16, 16]) torch.bool\n"
     ]
    }
   ],
   "source": [
    "def encode_src(text: str, vocab: Dict[str, Any], max_len: int) -> List[int]:\n",
    "    toks = tokenize(normalize(text, LOWERCASE))\n",
    "    toks = toks[:max_len]\n",
    "    ids = [vocab[\"stoi\"].get(t, vocab[\"stoi\"][UNK]) for t in toks]\n",
    "    ids.append(vocab[\"stoi\"][EOS])\n",
    "    return ids\n",
    "\n",
    "def encode_tgt(text: str, vocab: Dict[str, Any], max_len: int) -> List[int]:\n",
    "    toks = tokenize(normalize(text, LOWERCASE))\n",
    "    toks = toks[:max_len]\n",
    "    toks = [BOS] + toks + [EOS]\n",
    "    ids = [vocab[\"stoi\"].get(t, vocab[\"stoi\"][UNK]) for t in toks]\n",
    "    return ids\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs: List[Tuple[str, str]]):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en, es = self.pairs[idx]\n",
    "        src_ids = encode_src(en, src_vocab, MAX_LEN_SRC)\n",
    "        tgt_ids = encode_tgt(es, tgt_vocab, MAX_LEN_TGT)\n",
    "        return src_ids, tgt_ids\n",
    "\n",
    "def make_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "    return torch.triu(torch.ones((sz, sz), dtype=torch.bool), diagonal=1)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_seqs, tgt_seqs = zip(*batch)\n",
    "    B = len(batch)\n",
    "\n",
    "    src_lens = [len(s) for s in src_seqs]\n",
    "    tgt_lens = [len(t) for t in tgt_seqs]\n",
    "\n",
    "    max_src = max(src_lens)\n",
    "    max_tgt = max(tgt_lens)\n",
    "\n",
    "    src = torch.full((B, max_src), pad_id_src, dtype=torch.long)\n",
    "    tgt = torch.full((B, max_tgt), pad_id_tgt, dtype=torch.long)\n",
    "\n",
    "    for i, (s, t) in enumerate(zip(src_seqs, tgt_seqs)):\n",
    "        src[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
    "        tgt[i, :len(t)] = torch.tensor(t, dtype=torch.long)\n",
    "\n",
    "    tgt_in  = tgt[:, :-1].contiguous()\n",
    "    tgt_out = tgt[:, 1:].contiguous()\n",
    "\n",
    "    src_key_padding_mask = (src == pad_id_src)\n",
    "    tgt_key_padding_mask = (tgt_in == pad_id_tgt)\n",
    "\n",
    "    Tt = tgt_in.size(1)\n",
    "    tgt_subsequent_mask = make_subsequent_mask(Tt)\n",
    "\n",
    "    return {\n",
    "        \"src\": src,\n",
    "        \"tgt_in\": tgt_in,\n",
    "        \"tgt_out\": tgt_out,\n",
    "        \"src_lens\": torch.tensor(src_lens, dtype=torch.long),\n",
    "        \"tgt_lens\": torch.tensor(tgt_lens, dtype=torch.long),\n",
    "        \"src_key_padding_mask\": src_key_padding_mask,\n",
    "        \"tgt_key_padding_mask\": tgt_key_padding_mask,\n",
    "        \"tgt_subsequent_mask\": tgt_subsequent_mask,\n",
    "    }\n",
    "\n",
    "train_dl = DataLoader(TranslationDataset(train_pairs), batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, collate_fn=collate_fn, drop_last=True)\n",
    "val_dl   = DataLoader(TranslationDataset(val_pairs), batch_size=BATCH_SIZE, shuffle=False,\n",
    "                      num_workers=NUM_WORKERS, collate_fn=collate_fn)\n",
    "test_dl  = DataLoader(TranslationDataset(test_pairs), batch_size=BATCH_SIZE, shuffle=False,\n",
    "                      num_workers=NUM_WORKERS, collate_fn=collate_fn)\n",
    "\n",
    "batch = next(iter(train_dl))\n",
    "print(\"src:\", batch[\"src\"].shape)\n",
    "print(\"tgt_in:\", batch[\"tgt_in\"].shape)\n",
    "print(\"tgt_out:\", batch[\"tgt_out\"].shape)\n",
    "print(\"src_key_padding_mask:\", batch[\"src_key_padding_mask\"].shape, batch[\"src_key_padding_mask\"].dtype)\n",
    "print(\"tgt_key_padding_mask:\", batch[\"tgt_key_padding_mask\"].shape, batch[\"tgt_key_padding_mask\"].dtype)\n",
    "print(\"tgt_subsequent_mask:\", batch[\"tgt_subsequent_mask\"].shape, batch[\"tgt_subsequent_mask\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e727d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, pad_idx: int):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.scale = math.sqrt(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.emb(x) * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "185172c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 512, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        T = x.size(1)\n",
    "        x = x + self.pe[:, :T, :].to(x.dtype)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1c0321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerInputEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, pad_idx: int, max_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.tok = TokenEmbedding(vocab_size, d_model, pad_idx)\n",
    "        self.pos = SinusoidalPositionalEncoding(d_model, max_len=max_len, dropout=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.pos(self.tok(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13902f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src ids: torch.Size([64, 18]) -> src embed: torch.Size([64, 18, 256])\n",
      "tgt ids: torch.Size([64, 16]) -> tgt embed: torch.Size([64, 16, 256])\n"
     ]
    }
   ],
   "source": [
    "src_embed = TransformerInputEmbedding(\n",
    "    vocab_size=len(src_vocab[\"itos\"]),\n",
    "    d_model=D_MODEL,\n",
    "    pad_idx=pad_id_src,\n",
    "    max_len=MAX_LEN,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "tgt_embed = TransformerInputEmbedding(\n",
    "    vocab_size=len(tgt_vocab[\"itos\"]),\n",
    "    d_model=D_MODEL,\n",
    "    pad_idx=pad_id_tgt,\n",
    "    max_len=MAX_LEN,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "batch = next(iter(train_dl))\n",
    "src = batch[\"src\"].to(DEVICE)\n",
    "tgt_in = batch[\"tgt_in\"].to(DEVICE)\n",
    "\n",
    "src_x = src_embed(src)\n",
    "tgt_x = tgt_embed(tgt_in)\n",
    "\n",
    "print(\"src ids:\", src.shape, \"-> src embed:\", src_x.shape)\n",
    "print(\"tgt ids:\", tgt_in.shape, \"-> tgt embed:\", tgt_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bc2c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    Q: torch.Tensor,\n",
    "    K: torch.Tensor,\n",
    "    V: torch.Tensor,\n",
    "    attn_mask: Optional[torch.Tensor] = None,\n",
    "    key_padding_mask: Optional[torch.Tensor] = None,\n",
    "    dropout_p: float = 0.0,\n",
    "    training: bool = False,\n",
    "):\n",
    "    Dh = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Dh)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dim() == 2:\n",
    "            scores = scores.masked_fill(attn_mask.unsqueeze(0).unsqueeze(0), float(\"-inf\"))\n",
    "        else:\n",
    "            scores = scores.masked_fill(attn_mask, float(\"-inf\"))\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        scores = scores.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(1), float(\"-inf\"))\n",
    "\n",
    "    attn = torch.softmax(scores, dim=-1)\n",
    "    if dropout_p > 0.0:\n",
    "        attn = F.dropout(attn, p=dropout_p, training=training)\n",
    "\n",
    "    out = torch.matmul(attn, V)\n",
    "    return out, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7a2ad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.Wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wk = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wv = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wo = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def _split_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, D = x.shape\n",
    "        x = x.view(B, T, self.num_heads, self.d_head)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def _merge_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, H, T, Dh = x.shape\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        return x.view(B, T, H * Dh)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        q: torch.Tensor,\n",
    "        k: torch.Tensor,\n",
    "        v: torch.Tensor,\n",
    "        attn_mask: Optional[torch.Tensor] = None,\n",
    "        key_padding_mask: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        Q = self._split_heads(self.Wq(q))\n",
    "        K = self._split_heads(self.Wk(k))\n",
    "        V = self._split_heads(self.Wv(v))\n",
    "\n",
    "        out_h, attn = scaled_dot_product_attention(\n",
    "            Q, K, V,\n",
    "            attn_mask=attn_mask,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            dropout_p=self.dropout,\n",
    "            training=self.training\n",
    "        )\n",
    "\n",
    "        out = self.Wo(self._merge_heads(out_h))\n",
    "        return out, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "454f546f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without causal mask -> out: torch.Size([2, 4, 8]) attn: torch.Size([2, 2, 4, 6])\n",
      "With causal mask -> out: torch.Size([2, 4, 8]) attn: torch.Size([2, 2, 4, 6])\n",
      "attn batch0 head0 last2 keys (should be ~0): tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "B, Tq, Tk, D, H = 2, 4, 6, 8, 2\n",
    "mha = MultiHeadAttention(d_model=D, num_heads=H, dropout=0.0).to(DEVICE)\n",
    "\n",
    "q = torch.randn(B, Tq, D, device=DEVICE)\n",
    "k = torch.randn(B, Tk, D, device=DEVICE)\n",
    "v = torch.randn(B, Tk, D, device=DEVICE)\n",
    "\n",
    "key_padding_mask = torch.zeros(B, Tk, dtype=torch.bool, device=DEVICE)\n",
    "key_padding_mask[0, -2:] = True\n",
    "\n",
    "attn_mask = torch.triu(torch.ones((Tq, Tk), dtype=torch.bool, device=DEVICE), diagonal=1)\n",
    "\n",
    "out, attn = mha(q, k, v, attn_mask=None, key_padding_mask=key_padding_mask)\n",
    "print(\"Without causal mask -> out:\", out.shape, \"attn:\", attn.shape)\n",
    "\n",
    "out2, attn2 = mha(q, k, v, attn_mask=attn_mask, key_padding_mask=key_padding_mask)\n",
    "print(\"With causal mask -> out:\", out2.shape, \"attn:\", attn2.shape)\n",
    "\n",
    "print(\"attn batch0 head0 last2 keys (should be ~0):\", attn[0,0,0,-2:].detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb2f06b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_out: torch.Size([64, 18, 64]) enc_attn: torch.Size([64, 8, 18, 18])\n",
      "dec_out: torch.Size([64, 20, 64]) dec_attn: torch.Size([64, 8, 20, 20])\n",
      "cross_out: torch.Size([64, 20, 64]) cross_attn: torch.Size([64, 8, 20, 18])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dl))\n",
    "src = batch[\"src\"].to(DEVICE)\n",
    "tgt_in = batch[\"tgt_in\"].to(DEVICE)\n",
    "\n",
    "D_MODEL = 64\n",
    "NUM_HEADS = 8\n",
    "\n",
    "emb_src = nn.Embedding(int(src.max().item()) + 1, D_MODEL).to(DEVICE)\n",
    "emb_tgt = nn.Embedding(int(tgt_in.max().item()) + 1, D_MODEL).to(DEVICE)\n",
    "\n",
    "src_x = emb_src(src)\n",
    "tgt_x = emb_tgt(tgt_in)\n",
    "\n",
    "mha = MultiHeadAttention(d_model=D_MODEL, num_heads=NUM_HEADS, dropout=0.0).to(DEVICE)\n",
    "\n",
    "src_pad_mask = batch[\"src_key_padding_mask\"].to(DEVICE)\n",
    "tgt_pad_mask = batch[\"tgt_key_padding_mask\"].to(DEVICE)\n",
    "tgt_causal = batch[\"tgt_subsequent_mask\"].to(DEVICE)\n",
    "\n",
    "enc_out, enc_attn = mha(src_x, src_x, src_x, attn_mask=None, key_padding_mask=src_pad_mask)\n",
    "print(\"enc_out:\", enc_out.shape, \"enc_attn:\", enc_attn.shape)\n",
    "\n",
    "dec_out, dec_attn = mha(tgt_x, tgt_x, tgt_x, attn_mask=tgt_causal, key_padding_mask=tgt_pad_mask)\n",
    "print(\"dec_out:\", dec_out.shape, \"dec_attn:\", dec_attn.shape)\n",
    "\n",
    "cross_out, cross_attn = mha(tgt_x, src_x, src_x, attn_mask=None, key_padding_mask=src_pad_mask)\n",
    "print(\"cross_out:\", cross_out.shape, \"cross_attn:\", cross_attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "891e1a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb59dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout=dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffn = PositionwiseFFN(d_model, d_ff=d_ff, dropout=dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_key_padding_mask=None):\n",
    "        attn_out, _ = self.self_attn(\n",
    "            src, src, src,\n",
    "            attn_mask=None,\n",
    "            key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        src = self.norm1(src + self.dropout(attn_out))\n",
    "\n",
    "        ffn_out = self.ffn(src)\n",
    "        src = self.norm2(src + self.dropout(ffn_out))\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c66ee20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, src, src_key_padding_mask=None):\n",
    "        out = src\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, src_key_padding_mask=src_key_padding_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "256acf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([2, 10, 32])\n",
      "Output: torch.Size([2, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "B, T, D = 2, 10, 32\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "encoder = TransformerEncoder(\n",
    "    num_layers=NUM_LAYERS,\n",
    "    d_model=D,\n",
    "    num_heads=NUM_HEADS,\n",
    "    d_ff=128,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "x = torch.randn(B, T, D).to(DEVICE)\n",
    "\n",
    "pad_mask = torch.zeros(B, T, dtype=torch.bool).to(DEVICE)\n",
    "pad_mask[0, -3:] = True\n",
    "\n",
    "out = encoder(x, src_key_padding_mask=pad_mask)\n",
    "print(\"Input:\", x.shape)\n",
    "print(\"Output:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8737c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout=dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffn = PositionwiseFFN(d_model, d_ff=d_ff, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt,\n",
    "        memory,\n",
    "        tgt_subsequent_mask=None,\n",
    "        tgt_key_padding_mask=None,\n",
    "        memory_key_padding_mask=None,\n",
    "    ):\n",
    "        self_out, _ = self.self_attn(\n",
    "            tgt, tgt, tgt,\n",
    "            attn_mask=tgt_subsequent_mask,\n",
    "            key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        tgt = self.norm1(tgt + self.dropout(self_out))\n",
    "\n",
    "        cross_out, _ = self.cross_attn(\n",
    "            tgt, memory, memory,\n",
    "            attn_mask=None,\n",
    "            key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        tgt = self.norm2(tgt + self.dropout(cross_out))\n",
    "\n",
    "        ffn_out = self.ffn(tgt)\n",
    "        tgt = self.norm3(tgt + self.dropout(ffn_out))\n",
    "\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f45c5145",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, num_heads, d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt,\n",
    "        memory,\n",
    "        tgt_subsequent_mask=None,\n",
    "        tgt_key_padding_mask=None,\n",
    "        memory_key_padding_mask=None,\n",
    "    ):\n",
    "        out = tgt\n",
    "        for layer in self.layers:\n",
    "            out = layer(\n",
    "                out,\n",
    "                memory,\n",
    "                tgt_subsequent_mask=tgt_subsequent_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                memory_key_padding_mask=memory_key_padding_mask,\n",
    "            )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8eec7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt: torch.Size([2, 7, 32])\n",
      "memory: torch.Size([2, 9, 32])\n",
      "out: torch.Size([2, 7, 32])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "B, Ts, Tt, D = 2, 9, 7, 32\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "decoder = TransformerDecoder(\n",
    "    num_layers=NUM_LAYERS,\n",
    "    d_model=D,\n",
    "    num_heads=NUM_HEADS,\n",
    "    d_ff=128,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "tgt = torch.randn(B, Tt, D, device=DEVICE)\n",
    "memory = torch.randn(B, Ts, D, device=DEVICE)\n",
    "\n",
    "tgt_sub = torch.triu(torch.ones((Tt, Tt), dtype=torch.bool, device=DEVICE), diagonal=1)\n",
    "tgt_pad = torch.zeros(B, Tt, dtype=torch.bool, device=DEVICE); tgt_pad[0, -2:] = True\n",
    "src_pad = torch.zeros(B, Ts, dtype=torch.bool, device=DEVICE); src_pad[1, -3:] = True\n",
    "\n",
    "out = decoder(\n",
    "    tgt,\n",
    "    memory,\n",
    "    tgt_subsequent_mask=tgt_sub,\n",
    "    tgt_key_padding_mask=tgt_pad,\n",
    "    memory_key_padding_mask=src_pad\n",
    ")\n",
    "\n",
    "print(\"tgt:\", tgt.shape)\n",
    "print(\"memory:\", memory.shape)\n",
    "print(\"out:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e99868f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        pad_id_src,\n",
    "        pad_id_tgt,\n",
    "        d_model=256,\n",
    "        num_heads=8,\n",
    "        num_layers=4,\n",
    "        d_ff=1024,\n",
    "        dropout=0.1,\n",
    "        max_len=256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_embed = TransformerInputEmbedding(\n",
    "            src_vocab_size, d_model, pad_id_src, max_len, dropout\n",
    "        )\n",
    "        self.tgt_embed = TransformerInputEmbedding(\n",
    "            tgt_vocab_size, d_model, pad_id_tgt, max_len, dropout\n",
    "        )\n",
    "\n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_layers=num_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            d_ff=d_ff,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.decoder = TransformerDecoder(\n",
    "            num_layers=num_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            d_ff=d_ff,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.out_proj = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        self.pad_id_src = pad_id_src\n",
    "        self.pad_id_tgt = pad_id_tgt\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        tgt_in,\n",
    "        src_key_padding_mask=None,\n",
    "        tgt_key_padding_mask=None,\n",
    "        tgt_subsequent_mask=None,\n",
    "    ):\n",
    "        src_x = self.src_embed(src)\n",
    "        tgt_x = self.tgt_embed(tgt_in)\n",
    "\n",
    "        memory = self.encoder(\n",
    "            src_x,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "\n",
    "        dec_out = self.decoder(\n",
    "            tgt_x,\n",
    "            memory,\n",
    "            tgt_subsequent_mask=tgt_subsequent_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "\n",
    "        logits = self.out_proj(dec_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "264a053a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 9.509612 M\n"
     ]
    }
   ],
   "source": [
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4\n",
    "D_FF = 1024\n",
    "DROPOUT = 0.1\n",
    "MAX_LEN = 256\n",
    "\n",
    "model = TransformerSeq2Seq(\n",
    "    src_vocab_size=len(src_vocab[\"itos\"]),\n",
    "    tgt_vocab_size=len(tgt_vocab[\"itos\"]),\n",
    "    pad_id_src=pad_id_src,\n",
    "    pad_id_tgt=pad_id_tgt,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    d_ff=D_FF,\n",
    "    dropout=DROPOUT,\n",
    "    max_len=MAX_LEN,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(\"Model parameters:\", sum(p.numel() for p in model.parameters())/1e6, \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba4c0028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: torch.Size([64, 27])\n",
      "tgt_in: torch.Size([64, 37])\n",
      "logits: torch.Size([64, 37, 3052])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dl))\n",
    "\n",
    "src = batch[\"src\"].to(DEVICE)\n",
    "tgt_in = batch[\"tgt_in\"].to(DEVICE)\n",
    "\n",
    "src_pad_mask = batch[\"src_key_padding_mask\"].to(DEVICE)\n",
    "tgt_pad_mask = batch[\"tgt_key_padding_mask\"].to(DEVICE)\n",
    "tgt_causal = batch[\"tgt_subsequent_mask\"].to(DEVICE)\n",
    "\n",
    "logits = model(\n",
    "    src,\n",
    "    tgt_in,\n",
    "    src_key_padding_mask=src_pad_mask,\n",
    "    tgt_key_padding_mask=tgt_pad_mask,\n",
    "    tgt_subsequent_mask=tgt_causal\n",
    ")\n",
    "\n",
    "print(\"src:\", src.shape)\n",
    "print(\"tgt_in:\", tgt_in.shape)\n",
    "print(\"logits:\", logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e98401d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = Path(\"outputs_transformer\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CKPT_DIR = OUT_DIR / \"checkpoints\"\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_PATH = OUT_DIR / \"run_log.json\"\n",
    "\n",
    "def append_jsonl(path: Path, obj: dict):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def safe_update_json(path: Path, key: str, value: dict):\n",
    "    if path.exists():\n",
    "        try:\n",
    "            data = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            data = {}\n",
    "    else:\n",
    "        data = {}\n",
    "\n",
    "    data[key] = value\n",
    "    path.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58a942b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    epochs: int = 50\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.0\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "cfg = TrainConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98afa62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id_tgt)\n",
    "\n",
    "opt = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=cfg.lr,\n",
    "    weight_decay=cfg.weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba5cb9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dl, opt, loss_fn, grad_clip: float):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for batch in dl:\n",
    "        src = batch[\"src\"].to(DEVICE)\n",
    "        tgt_in = batch[\"tgt_in\"].to(DEVICE)\n",
    "        tgt_out = batch[\"tgt_out\"].to(DEVICE)\n",
    "\n",
    "        src_pad_mask = batch[\"src_key_padding_mask\"].to(DEVICE)\n",
    "        tgt_pad_mask = batch[\"tgt_key_padding_mask\"].to(DEVICE)\n",
    "        tgt_causal = batch[\"tgt_subsequent_mask\"].to(DEVICE)\n",
    "\n",
    "        logits = model(\n",
    "            src, tgt_in,\n",
    "            src_key_padding_mask=src_pad_mask,\n",
    "            tgt_key_padding_mask=tgt_pad_mask,\n",
    "            tgt_subsequent_mask=tgt_causal\n",
    "        )\n",
    "        loss = loss_fn(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        steps += 1\n",
    "\n",
    "    return total_loss / max(steps, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, dl, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for batch in dl:\n",
    "        src = batch[\"src\"].to(DEVICE)\n",
    "        tgt_in = batch[\"tgt_in\"].to(DEVICE)\n",
    "        tgt_out = batch[\"tgt_out\"].to(DEVICE)\n",
    "\n",
    "        src_pad_mask = batch[\"src_key_padding_mask\"].to(DEVICE)\n",
    "        tgt_pad_mask = batch[\"tgt_key_padding_mask\"].to(DEVICE)\n",
    "        tgt_causal = batch[\"tgt_subsequent_mask\"].to(DEVICE)\n",
    "\n",
    "        logits = model(\n",
    "            src, tgt_in,\n",
    "            src_key_padding_mask=src_pad_mask,\n",
    "            tgt_key_padding_mask=tgt_pad_mask,\n",
    "            tgt_subsequent_mask=tgt_causal\n",
    "        )\n",
    "\n",
    "        loss = loss_fn(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        steps += 1\n",
    "\n",
    "    return total_loss / max(steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd3a44e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 01 | train 4.8136 | val 3.8597 | time 105.9s\n",
      "Epoch 02 | train 3.8265 | val 3.3927 | time 39.4s\n",
      "Epoch 03 | train 3.3822 | val 3.1408 | time 33.3s\n",
      "Epoch 04 | train 3.0538 | val 2.9318 | time 52.2s\n",
      "Epoch 05 | train 2.7713 | val 2.7202 | time 26.4s\n",
      "Epoch 06 | train 2.5186 | val 2.5867 | time 28.5s\n",
      "Epoch 07 | train 2.2932 | val 2.4803 | time 25.2s\n",
      "Epoch 08 | train 2.0807 | val 2.4074 | time 25.9s\n",
      "Epoch 09 | train 1.8863 | val 2.3346 | time 21.5s\n",
      "Epoch 10 | train 1.6965 | val 2.2839 | time 22.1s\n",
      "Epoch 11 | train 1.5261 | val 2.2515 | time 21.6s\n",
      "Epoch 12 | train 1.3670 | val 2.2669 | time 21.3s\n",
      "Epoch 13 | train 1.2145 | val 2.2031 | time 21.4s\n",
      "Epoch 14 | train 1.0763 | val 2.2334 | time 21.4s\n",
      "Epoch 15 | train 0.9676 | val 2.1965 | time 20.9s\n",
      "Epoch 16 | train 0.8499 | val 2.2560 | time 22.2s\n",
      "Epoch 17 | train 0.7498 | val 2.3235 | time 22.7s\n",
      "Epoch 18 | train 0.6567 | val 2.3107 | time 20.8s\n",
      "Epoch 19 | train 0.5851 | val 2.3496 | time 21.2s\n",
      "Epoch 20 | train 0.5165 | val 2.3653 | time 21.1s\n",
      "Epoch 21 | train 0.4554 | val 2.4170 | time 21.0s\n",
      "Epoch 22 | train 0.4173 | val 2.3892 | time 22.6s\n",
      "Epoch 23 | train 0.3702 | val 2.4731 | time 21.0s\n",
      "Epoch 24 | train 0.3361 | val 2.5006 | time 20.7s\n",
      "Epoch 25 | train 0.3123 | val 2.5172 | time 21.3s\n",
      "Epoch 26 | train 0.2828 | val 2.5404 | time 21.8s\n",
      "Epoch 27 | train 0.2667 | val 2.5780 | time 20.9s\n",
      "Epoch 28 | train 0.2459 | val 2.6333 | time 34.9s\n",
      "Epoch 29 | train 0.2289 | val 2.6475 | time 35.3s\n",
      "Epoch 30 | train 0.2114 | val 2.6432 | time 25.4s\n",
      "Epoch 31 | train 0.2018 | val 2.7097 | time 35.6s\n",
      "Epoch 32 | train 0.1910 | val 2.7002 | time 25.8s\n",
      "Epoch 33 | train 0.1803 | val 2.7439 | time 25.2s\n",
      "Epoch 34 | train 0.1752 | val 2.7875 | time 24.8s\n",
      "Epoch 35 | train 0.1614 | val 2.7899 | time 27.8s\n",
      "Epoch 36 | train 0.1584 | val 2.8675 | time 26.2s\n",
      "Epoch 37 | train 0.1565 | val 2.8602 | time 24.5s\n",
      "Epoch 38 | train 0.1480 | val 2.9133 | time 21.8s\n",
      "Epoch 39 | train 0.1457 | val 2.9019 | time 21.7s\n",
      "Epoch 40 | train 0.1358 | val 2.9046 | time 22.0s\n",
      "Epoch 41 | train 0.1325 | val 2.9486 | time 21.4s\n",
      "Epoch 42 | train 0.1301 | val 2.9544 | time 20.6s\n",
      "Epoch 43 | train 0.1252 | val 2.9399 | time 21.0s\n",
      "Epoch 44 | train 0.1199 | val 2.9400 | time 21.4s\n",
      "Epoch 45 | train 0.1180 | val 2.9395 | time 20.5s\n",
      "Epoch 46 | train 0.1169 | val 2.9943 | time 21.3s\n",
      "Epoch 47 | train 0.1106 | val 3.0111 | time 21.0s\n",
      "Epoch 48 | train 0.1119 | val 3.0440 | time 26.6s\n",
      "Epoch 49 | train 0.1072 | val 3.0462 | time 26.1s\n",
      "Epoch 50 | train 0.1042 | val 3.0823 | time 21.5s\n",
      "Training finished. Total time: 1319.4s\n",
      "Best checkpoint: outputs_transformer/checkpoints/transformer_best.pt\n",
      "Logs: outputs_transformer/run_log.jsonl and outputs_transformer/run_log.json\n"
     ]
    }
   ],
   "source": [
    "best_val = float(\"inf\")\n",
    "best_path = CKPT_DIR / \"transformer_best.pt\"\n",
    "\n",
    "run_id = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "history = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "t0_global = time.time()\n",
    "\n",
    "for epoch in range(1, cfg.epochs + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_loss = train_one_epoch(model, train_dl, opt, loss_fn, cfg.grad_clip)\n",
    "    val_loss = eval_one_epoch(model, val_dl, loss_fn)\n",
    "\n",
    "    epoch_time = time.time() - t0\n",
    "\n",
    "    improved = val_loss < best_val\n",
    "    if improved:\n",
    "        best_val = val_loss\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"config\": {\n",
    "                    \"d_model\": getattr(model, \"d_model\", None),\n",
    "                    \"num_layers\": getattr(model, \"num_layers\", None),\n",
    "                    \"num_heads\": getattr(model, \"num_heads\", None),\n",
    "                },\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "            },\n",
    "            best_path\n",
    "        )\n",
    "\n",
    "    row = {\n",
    "        \"run_id\": run_id,\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"epoch_time_sec\": epoch_time,\n",
    "        \"best_val_loss_so_far\": best_val,\n",
    "        \"saved_best\": bool(improved),\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    history.append(row)\n",
    "\n",
    "    append_jsonl(OUT_DIR / \"run_log.jsonl\", row)\n",
    "    safe_update_json(LOG_PATH, f\"epoch_{epoch}\", row)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train {train_loss:.4f} | val {val_loss:.4f} | time {epoch_time:.1f}s\")\n",
    "\n",
    "t_total = time.time() - t0_global\n",
    "print(f\"Training finished. Total time: {t_total:.1f}s\")\n",
    "print(\"Best checkpoint:\", best_path)\n",
    "print(\"Logs:\", OUT_DIR / \"run_log.jsonl\", \"and\", LOG_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7853108b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from epoch: 15\n"
     ]
    }
   ],
   "source": [
    "CKPT_PATH = Path(\"outputs_transformer/checkpoints/transformer_best.pt\")\n",
    "\n",
    "assert CKPT_PATH.exists(), f\"Checkpoint not found: {CKPT_PATH}\"\n",
    "\n",
    "ckpt = torch.load(CKPT_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded checkpoint from epoch:\", ckpt.get(\"epoch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd145088",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decode_transformer(model, src, src_pad_mask, bos_id, eos_id, max_len=40):\n",
    "    B = src.size(0)\n",
    "    ys = torch.full((B,1), bos_id, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        tgt_pad_mask = (ys == pad_id_tgt)\n",
    "\n",
    "        T = ys.size(1)\n",
    "        tgt_causal = torch.triu(\n",
    "            torch.ones((T, T), dtype=torch.bool, device=DEVICE),\n",
    "            diagonal=1\n",
    "        )\n",
    "\n",
    "        logits = model(\n",
    "            src,\n",
    "            ys,\n",
    "            src_key_padding_mask=src_pad_mask,\n",
    "            tgt_key_padding_mask=tgt_pad_mask,\n",
    "            tgt_subsequent_mask=tgt_causal\n",
    "        )\n",
    "\n",
    "        next_token = logits[:, -1].argmax(dim=-1, keepdim=True)\n",
    "        ys = torch.cat([ys, next_token], dim=1)\n",
    "\n",
    "        if (next_token == eos_id).all():\n",
    "            break\n",
    "\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38d39959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_sentence(ids, vocab):\n",
    "    tokens = []\n",
    "    for i in ids:\n",
    "        tok = vocab[\"itos\"][int(i)]\n",
    "        if tok == \"<eos>\":\n",
    "            break\n",
    "        if tok not in [\"<bos>\", \"<pad>\"]:\n",
    "            tokens.append(tok)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02a90c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding: 100%|██████████| 16/16 [00:46<00:00,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "REF: él es su único hijo .\n",
      "HYP: es solo niño .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "refs = []\n",
    "hyps = []\n",
    "\n",
    "bos_id_tgt = tgt_vocab[\"stoi\"][\"<bos>\"]\n",
    "eos_id_tgt = tgt_vocab[\"stoi\"][\"<eos>\"]\n",
    "\n",
    "for batch in tqdm(test_dl, desc=\"Decoding\"):\n",
    "    src = batch[\"src\"].to(DEVICE)\n",
    "    src_pad_mask = batch[\"src_key_padding_mask\"].to(DEVICE)\n",
    "    tgt_out = batch[\"tgt_out\"]\n",
    "\n",
    "    pred_ids = greedy_decode_transformer(\n",
    "        model,\n",
    "        src,\n",
    "        src_pad_mask,\n",
    "        bos_id_tgt,\n",
    "        eos_id_tgt,\n",
    "        max_len=40\n",
    "    )\n",
    "\n",
    "    for ref_seq, hyp_seq in zip(tgt_out, pred_ids.cpu()):\n",
    "        ref = ids_to_sentence(ref_seq, tgt_vocab)\n",
    "        hyp = ids_to_sentence(hyp_seq, tgt_vocab)\n",
    "\n",
    "        refs.append(ref)\n",
    "        hyps.append(hyp)\n",
    "\n",
    "print(\"Example:\")\n",
    "print(\"REF:\", refs[0])\n",
    "print(\"HYP:\", hyps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a1296dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 28.755664016702184\n"
     ]
    }
   ],
   "source": [
    "bleu = sacrebleu.corpus_bleu(hyps, [refs], tokenize=\"13a\", force=True)\n",
    "print(\"BLEU:\", bleu.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0800196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "- outputs_transformer/transformer_predictions.tsv\n",
      "- outputs_transformer/transformer_bleu.json\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = Path(\"outputs_transformer\")\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "pred_path = OUT_DIR / \"transformer_predictions.tsv\"\n",
    "metrics_path = OUT_DIR / \"transformer_bleu.json\"\n",
    "\n",
    "with open(pred_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r, h in zip(refs, hyps):\n",
    "        f.write(r + \"\\t\" + h + \"\\n\")\n",
    "\n",
    "with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"bleu\": bleu.score,\n",
    "        \"num_sentences\": len(hyps)\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"-\", pred_path)\n",
    "print(\"-\", metrics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be77ce05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
